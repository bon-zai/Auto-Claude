---
phase: 02-context-enrichment
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - apps/backend/runners/github/context_gatherer.py
autonomous: true

must_haves:
  truths:
    - "Related files limit is 50 instead of 20"
    - "Files are prioritized by relevance (tests > types > configs)"
    - "Reverse dependencies (files that import changed files) are detected"
    - "Performance is acceptable (< 5 seconds for typical PRs)"
  artifacts:
    - path: "apps/backend/runners/github/context_gatherer.py"
      provides: "Enhanced related files with prioritization and reverse deps"
      contains: "_find_dependents"
      contains: "_prioritize_related_files"
  key_links:
    - from: "_find_related_files()"
      to: "_find_dependents()"
      via: "method call for each changed file"
      pattern: "_find_dependents|_prioritize_related_files"
---

<objective>
Enhance related files gathering with increased limits, smart prioritization, and reverse dependency analysis.

Purpose: The current 20-file limit may miss important dependencies. Additionally, we only find what changed files import, not what imports them (reverse dependencies). Breaking changes to APIs or utilities may go undetected without knowing the callers.

Output: Smarter related files selection with up to 50 files, prioritized by relevance, including reverse dependencies.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/02-context-enrichment/02-RESEARCH.md
@apps/backend/runners/github/context_gatherer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add reverse dependency detection</name>
  <files>apps/backend/runners/github/context_gatherer.py</files>
  <action>
Add `import subprocess` at the top of the file (after other imports).

Add method `_find_dependents(self, file_path: str, max_results: int = 15) -> set[str]`:

```python
def _find_dependents(self, file_path: str, max_results: int = 15) -> set[str]:
    """
    Find files that import the given file (reverse dependencies).

    Uses grep to search for import statements referencing this file.
    Limited to prevent performance issues on large codebases.

    Args:
        file_path: Path of the file to find dependents for
        max_results: Maximum number of dependents to return

    Returns:
        Set of file paths that import this file.
    """
    dependents: set[str] = set()
    path_obj = Path(file_path)
    stem = path_obj.stem  # e.g., 'helpers' from 'utils/helpers.ts'

    # Skip if stem is too generic (would match too many files)
    if stem in ["index", "main", "app", "utils", "helpers", "types", "constants"]:
        return dependents

    # Build grep patterns based on file type
    patterns = []

    if path_obj.suffix in [".ts", ".tsx", ".js", ".jsx"]:
        # Match various import styles for JS/TS
        # from './helpers', from '../utils/helpers', from '@/utils/helpers'
        patterns.append(f"['\"].*{stem}['\"]")
        # Also match without extension: import from './helpers' (no .ts)
        file_types = ["--include=*.ts", "--include=*.tsx", "--include=*.js", "--include=*.jsx"]
    elif path_obj.suffix == ".py":
        # Match Python imports: from .helpers import, import helpers
        patterns.append(f"(from.*{stem}|import.*{stem})")
        file_types = ["--include=*.py"]
    else:
        return dependents

    try:
        for pattern in patterns:
            result = subprocess.run(
                [
                    "grep",
                    "-rl",  # Recursive, list files only
                    "-E",   # Extended regex
                    pattern,
                    *file_types,
                    "--exclude-dir=node_modules",
                    "--exclude-dir=.git",
                    "--exclude-dir=dist",
                    "--exclude-dir=build",
                    "--exclude-dir=__pycache__",
                    "--exclude-dir=.venv",
                    "--exclude-dir=venv",
                    ".",
                ],
                cwd=self.project_dir,
                capture_output=True,
                text=True,
                timeout=5.0,  # 5 second timeout to prevent hanging
            )
            if result.returncode == 0:
                for line in result.stdout.strip().split("\n"):
                    if line and line != f"./{file_path}":
                        # Remove leading ./
                        clean_path = line.lstrip("./")
                        # Don't include the file itself
                        if clean_path != file_path:
                            dependents.add(clean_path)
                            if len(dependents) >= max_results:
                                return dependents
    except subprocess.TimeoutExpired:
        safe_print(f"[Context] Timeout finding dependents for {file_path}")
    except FileNotFoundError:
        # grep not available - skip silently
        pass
    except Exception as e:
        safe_print(f"[Context] Error finding dependents: {e}")

    return dependents
```

Key points:
- Skip generic names that would match too many files
- Use file type filters to reduce search space
- Exclude common non-code directories
- 5-second timeout prevents hanging on large repos
- Limit results to prevent overwhelming the context
  </action>
  <verify>
Test manually:
```bash
cd /Users/andremikalsen/Documents/Coding/autonomous-coding/apps/backend && python -c "
from runners.github.context_gatherer import PRContextGatherer
from pathlib import Path

gatherer = PRContextGatherer(Path('../..'), 1)
# Find files that import context_gatherer.py
dependents = gatherer._find_dependents('apps/backend/runners/github/context_gatherer.py')
print(f'Found {len(dependents)} dependents:')
for dep in sorted(dependents)[:10]:
    print(f'  - {dep}')
"
```
  </verify>
  <done>
_find_dependents() finds files that import a given file using grep search with timeouts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add smart file prioritization</name>
  <files>apps/backend/runners/github/context_gatherer.py</files>
  <action>
Add method `_prioritize_related_files(self, files: set[str], limit: int = 50) -> list[str]`:

```python
def _prioritize_related_files(self, files: set[str], limit: int = 50) -> list[str]:
    """
    Prioritize related files by relevance.

    Priority order:
    1. Test files (most important for review context)
    2. Type definition files (.d.ts)
    3. Configuration files
    4. Direct imports/dependents
    5. Other files

    Args:
        files: Set of file paths to prioritize
        limit: Maximum number of files to return

    Returns:
        List of files sorted by priority, limited to `limit`.
    """
    test_files = []
    type_files = []
    config_files = []
    other_files = []

    for f in files:
        path = Path(f)
        name_lower = path.name.lower()

        # Test files
        if (
            ".test." in name_lower
            or ".spec." in name_lower
            or name_lower.startswith("test_")
            or name_lower.endswith("_test.py")
            or "__tests__" in f
        ):
            test_files.append(f)
        # Type definition files
        elif path.suffix == ".d.ts" or "types" in name_lower:
            type_files.append(f)
        # Config files
        elif name_lower in [n.lower() for n in CONFIG_FILE_NAMES] or name_lower.endswith((".config.js", ".config.ts", "rc", "rc.json")):
            config_files.append(f)
        else:
            other_files.append(f)

    # Combine in priority order
    prioritized = test_files + type_files + config_files + other_files

    # Sort within each category alphabetically for consistency
    return sorted(prioritized)[:limit]
```

This method ensures the most relevant files are included when we hit the limit.
  </action>
  <verify>
Test prioritization logic:
```python
files = {
    "src/utils.ts",
    "src/utils.test.ts",
    "src/utils.d.ts",
    "tsconfig.json",
    "src/index.ts"
}
result = gatherer._prioritize_related_files(files, limit=3)
# Should return test file, type file, then config
```
  </verify>
  <done>
_prioritize_related_files() sorts files by relevance: tests > types > configs > other.
  </done>
</task>

<task type="auto">
  <name>Task 3: Update _find_related_files with new features</name>
  <files>apps/backend/runners/github/context_gatherer.py</files>
  <action>
Modify `_find_related_files()` method to:
1. Increase limit from 20 to 50
2. Add reverse dependency detection
3. Use smart prioritization

Update the method (starting around line 822):

```python
def _find_related_files(self, changed_files: list[ChangedFile]) -> list[str]:
    """
    Find files related to the changes.

    This includes:
    - Test files for changed source files
    - Imported modules and dependencies
    - Configuration files in the same directory
    - Related type definition files
    - Reverse dependencies (files that import changed files)
    """
    related = set()

    for changed_file in changed_files:
        path = Path(changed_file.path)

        # Find test files
        related.update(self._find_test_files(path))

        # Find imported files (for supported languages)
        if path.suffix in [".ts", ".tsx", ".js", ".jsx", ".py"]:
            related.update(self._find_imports(changed_file.content, path))

        # Find config files in same directory
        related.update(self._find_config_files(path.parent))

        # Find type definition files
        if path.suffix in [".ts", ".tsx"]:
            related.update(self._find_type_definitions(path))

        # NEW: Find reverse dependencies (files that import this file)
        related.update(self._find_dependents(changed_file.path))

    # Remove files that are already in changed_files
    changed_paths = {cf.path for cf in changed_files}
    related = {r for r in related if r not in changed_paths}

    # Use smart prioritization with increased limit (50 instead of 20)
    return self._prioritize_related_files(related, limit=50)
```

Also update `find_related_files_for_root()` static method (around line 1039) to use limit=50:
```python
# Change the last line from:
return sorted(related)[:20]
# To:
return sorted(related)[:50]
```
  </action>
  <verify>
Run tests and verify no regressions:
```bash
cd /Users/andremikalsen/Documents/Coding/autonomous-coding && apps/backend/.venv/bin/pytest tests/ -v --tb=short 2>&1 | head -50
```

Manual test to verify increased limit and reverse deps:
```bash
cd /Users/andremikalsen/Documents/Coding/autonomous-coding/apps/backend && python -c "
from runners.github.context_gatherer import PRContextGatherer, ChangedFile
from pathlib import Path

gatherer = PRContextGatherer(Path('../..'), 1)
# Create a mock changed file
changed = ChangedFile(
    path='apps/backend/runners/github/context_gatherer.py',
    status='modified',
    additions=10,
    deletions=5,
    content=open('runners/github/context_gatherer.py').read(),
    base_content='',
    patch=''
)
related = gatherer._find_related_files([changed])
print(f'Found {len(related)} related files (max 50):')
for f in related[:15]:
    print(f'  - {f}')
"
```
  </verify>
  <done>
_find_related_files() now:
- Returns up to 50 files (was 20)
- Includes reverse dependencies
- Prioritizes test files, types, and configs
  </done>
</task>

</tasks>

<verification>
1. No test failures
2. Related files limit is 50
3. Reverse dependencies are detected
4. Files are prioritized by relevance
5. Performance is acceptable (grep has 5s timeout)
</verification>

<success_criteria>
- Related files limit increased from 20 to 50
- Files are prioritized: tests > types > configs > other
- Reverse dependencies detected for changed files
- Grep search has timeout protection (5 seconds)
- Generic file names (index, main) skipped in reverse dep search
</success_criteria>

<output>
After completion, create `.planning/phases/02-context-enrichment/02-03-SUMMARY.md`
</output>
